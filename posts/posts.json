[
  {
    "path": "posts/2023-04-24-movement-of-tenants-evicted-from-buildings-with-highest-eviction-filing-rates/",
    "title": "Movement of Tenants Evicted from Buildings with Highest Eviction Filing Rates",
    "description": "An introduction to my Summer 2022 internship experience with the Eviction Lab\non better understanding eviction practices in Philadelphia, PA.",
    "author": [
      {
        "name": "Sherry Huang",
        "url": {}
      }
    ],
    "date": "2023-04-24",
    "categories": [],
    "contents": "\nIntroduction\nIn Philadelphia, 25 addresses have accounted for 8.6 percent of all eviction\nfilings in the county from 2016 to 2020. Meanwhile, other buildings in the\nneighborhood file few evictions despite serving tenants with similar socioeconomic\nbackgrounds. The two current contending theories for these “top evictor” buildings\nare: 1) abusive management or 2) buildings of last resort. Abusive management is\nthe idea that property managers may be too quick to file evictions. If, however,\n“top evictors” are serving as buildings of last resort, then the high eviction\nrates could be explained by the tenants that are selecting into these buildings.\nWhile the tenantry of top evictors may have similar socioeconomic characteristics\nas the tenantry of more stable buildings, tenant self-selection may be selecting\non observable characteristics associated with housing instability.\nThis research brief examines the mobility of renters in Philadelphia County in\nand out of these top evictor buildings since 2016. The analysis aims to understand\nwhy these buildings have such high eviction filing rates, as forced mobility is\ndisruptive and traumatizing for renters.\nData and Methods\nThis analysis makes use of the 5-year American Community Survey (ACS) data\nfrom 2015-2019 for the data sets on race, median household income, median gross\nrent, and college degree. The Eviction Lab collected data on the top 25 buildings\nand zip codes with the highest eviction filing rates in Philadelphia County, PA,\nas well as data on mobility to and from select addresses. An eviction filing refers\nto any non-commercial eviction (i.e., residential housing not occupied by a business)\ncase for which we observe a court record.\nUsing Philadelphia County census tract shapefiles\nfound on Open Data Philly, we clustered the census\ntracts by race, median household income, median gross rent, and college degree\nusing Latent Profile Analysis. Using a “VVV model” (varying volume, varying\nshape, and varying orientation) with six clusters, the model returned a lower BIC\nvalue (-7785.103) than other models and almost all other numbers of clusters.\nClustering allows us to group census tracts with similar races, incomes, rents,\nand college degrees. We then combined the clustered data with the top evictor\nbuildings (geocoded with QGIS) in order to discover if the buildings are clustered\nin a way that is different from low eviction buildings.\nTo analyze mobility flows across numerous buildings and zip codes, we incorporated\naddress data from Infutor. We identified the most recent moves before and after\nliving in a top evictor building and/or zip code from the Infutor data. We also\nfound buildings nearby with similar demographics but lower eviction rates using data.\nUsing the epiflows\npackage in R, we then created and compared network graphs for each property and\nzip code with the data. The buildings or zip codes serve as nodes, and edges\nrepresent a shared tenant. All plots can be accessed at this interactive data visualization app.\nResults\nAcross the top 25 top evictor buildings, we find that most of the top evictor\nbuildings are found in areas with lower median household income and lower percentage\nof White residents. We also find that almost all the zip codes containing top\nevictor buildings are found among the top 25 zip codes with the highest eviction\nfiling rates. Our analysis thus suggests that eviction filings are deeply linked\nwith race and poverty.\nMapping the network graphs of top evictors, “control” buildings, and zip codes\nallows us to compare general mobility patterns across the different areas.\nPreliminary research shows that the top evicting buildings’ network graphs do not\ndiffer significantly from their low-evicting neighbors. Our analysis suggests that\nbecause these network graphs do not differ, then these top evictor buildings are\nnot serving as buildings of last resort.\nWhile our analysis does not show that “abusive management” is the cause of the\nhigh eviction rates in these top evictor buildings, it is our best guess given\nthe patterns that we are seeing. Thus, the lab is following up by collaborating\nwith a team of legal aid providers, who will be doing interviews with tenants of\ntop evictor buildings to learn more about management practices there. Understanding\nwhat differentiates these top evictors from their seemingly similar neighbors\nwould give policymakers leverage in reducing the toll of mass evictions on their\ncommunities. If abusive management is in fact the case in these top evictors, then\ntop-down enforcement actions may be effective at reducing top evictors’ contribution\nto city-level eviction rates.\nAcknowledgements\nI would like to thank the Lang Center for Civic and Social Responsibility\nfor fully funding this experience and allowing me to work with the Eviction Lab.\nI would also like to thank the Eviction Lab for allowing me to intern at their\nlab this past summer. I would especially like to thank Carl Gershenson for being my\nmentor throughout this project.\nReferences\nDesmond, Matthew. Evicted: Poverty and Profit in the American City. Crown Publishers, 2016.\nDesmond, Matthew, et al. “Forced Relocation and Residential Instability among\nUrban Renters.” Social Service Review, vol. 89, no. 2, June 2015, pp. 227–262.,\nhttps://doi.org/10.1086/681091.\nBadger, Emily. “When the Best Available Home Is the One You Already Have.” The\nNew York Times, The New York Times, 27 May 2022,\nhttps://www.nytimes.com/2022/05/27/upshot/housing-market-slow-moving.html.\n\n\n\n",
    "preview": {},
    "last_modified": "2023-04-24T14:27:39-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2022-07-16-imputation-bias-in-forensic-fingerprint-examinations/",
    "title": "Imputation Bias in Forensic Fingerprint Examinations",
    "description": "An introduction to my Summer 2021 research with Professor Amanda Luby on\nimputation bias in minutiae markings by fingerprint examiners.",
    "author": [
      {
        "name": "Sherry Huang",
        "url": {}
      }
    ],
    "date": "2022-07-16",
    "categories": [],
    "contents": "\nIntroduction\nImputation is the process of replacing missing data with substituted values. In\nthe forensic fingerprint examination process, this bias could come in during the\nstage of comparing two potential matching prints. Examiners are shown the latent\nand exemplar prints at the same time during this stage, and there is evidence of\nminutiae movement, addition, and/or deletion during this stage. In my Summer 2021\nresearch, I explored patterns in these movements in order to identify potential\nimputation bias in the forensic fingerprint examination process.\nMethods\nIn order to explore minutiae movement across variables such as examiners, prints,\nand feature types, I created an interactive visualization app\nas part of my Summer 2021 research with Professor Amanda Luby.\nThe app allows you to select pairs of prints via the Pair ID drop-down menu. Options\ninclude mates (variables starting with “M”) and non-mates (variables starting with\n“N”). The app also allows you to specify an examiner. On the right, the app displays\nthe minutiae markings of all the examiners who looked at the pair of prints (“Print”\ntab), or the minutiae markings of the specific examiner (“Examiner” tab), or the\nsummary of the analysis of the prints (“Summary” tab). In the Print and Examiner\ntabs, each point represents a minutiae marking by an examiner. Lines represent changes\nin their markings between stages of examination. Hexes represent the concentration\nof markings, where dark represents a higher concentration of markings in the area.\nThe Summary tab displays how many examiners looked at the pair of prints; which\npercentage of the examiners were correct, were inconclusive, changed minutiae markings;\nand the difficulty each examiner ranked the pair of prints.\nResults and Discussion\nAlthough moving minutiae can occur for a variety of reasons, imputation bias is\nstill prevalent when there are changes in minutiae. The concern with moving minutiae\nis that examiners could be doing so in order to confirm their preliminary conclusions.\nFor example, as shown by my app, examiners tend to move more minutiae in mated pairs,\nindicating that they may be changing more pre-marked minutiae from their first stage\nof examination in order to validate their individualization decisions. Additionally,\nexaminers who viewed the same latent print used in two different pairs have different\npreliminary markings, as well as different minutiae movements in the Comparison\nstage. This indicates that despite the use of the same latent print in both pairs,\nexaminers still mark different minutiae, and then choose to move them differently\nin the Comparison stage. The different minutiae movements indicates imputation bias,\nas they are choosing to move minutiae based on the exemplar print it is paired with.\nMany of the minutiae movement occur due to a lack of standardized procedure in the\nforensic fingerprint examination process. For example, more deletions might occur\nin the Comparison stage at an agency where there is a minimum minutiae requirement;\nexaminers might be unnecessarily marking minutiae in order to hit the minimum, and then\nlater deleting the ones that are not helpful. Imputation bias can lead to misidentifications,\nwhich can be very harmful in large, prominent criminal cases. Finally, as previous\npapers have mentioned, there should be a rigorously defined, standard forensic fingerprint\nexamination ACE-V process (Ulery et al., 2015). Many of the examiners, in the White\nBox study, stated that exclusions and inconclusive prints were more difficult when\nthey had to mark minutiae (which they were not required to do so in the Black Box\nstudy) (Ulery et al., 2015). This indicates that examiners do not necessarily have\nto mark minutiae in a standardized way. However, if they did, there might be more\ntransparency in the process, which is particularly important when errors occur.\nIn the studies, the consequences of making a false identification (or exclusion)\nare insignificant, but in true casework, the consequences could change many people’s\nlives for the worst.\nTerminology\nLatent print - An impression of the friction skin of the fingers or palms of the\nhands that has been transferred to another surface. These prints are typically lifted\nfrom crime scenes and are usually not of the highest quality.\nExemplar print - A print of the fingers and/or palms deliberately taken from an\nindividual for record-keeping purposes. Since these prints are collected under ideal\ncircumstances, they are usually high quality.\nMinutiae - Major features of a fingerprint image; they are used in the matching\nof fingerprints.\nMates - Two prints that came from the same source.\nNon-mates - Two prints that did not come from the same source.\nACE-V process - Method used to reach a determination on each print. ACE-V stands\nfor Analysis, Comparison, Evaluation, and Verification.\nAnalysis: The examiner assesses the exemplar print to determine if it\ncan be used for comparison. This step involves the preliminary assessment of\nseveral factors like the surface material, the substance of the print itself,\nand/or minutiae.\nComparison: The examiner is shown both the exemplar and a latent print.\nThe latent print may come from searching through the Automated Fingerprint Identification\nSystem (AFIS) or some other fingerprint database. The examiner analyses characteristic\nattributes of both prints and identifies conformities between the exemplar and\nthe latent prints.\nEvaluation: The examiner decides if the prints are from the same source\n(identification or individualization), different sources (exclusion), or are\ninconclusive.\nVerification: If an identification is made, the conclusion must be verified\nby another fingerprint examiner.\n\nReferences\nUlery, B. T., Hicklin, R. A., Buscaglia, J., and Roberts, M. A. (2011). Accuracy\nand reliability of forensic latent fingerprint decisions. Proceedings of the National\nAcademy of Sciences, 108(19):7733–7738.\nUlery, B. T., Hicklin, R. A., Roberts, M. A., and Buscaglia, J. (2014). Measuring\nwhat latent fingerprint examiners consider sufficient information for individualization\ndeterminations. PlOS ONE, 9(11):e110179.\nUlery, B. T., Hicklin, R. A., Roberts, M. A., and Buscaglia, J. (2015). Changes\nin latent fingerprint examiners’ markup between analysis and comparison. Forensic\nScience International, 247:54–61.\n\n\n\n",
    "preview": {},
    "last_modified": "2023-05-15T14:50:55-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-06-30-bias/",
    "title": "Bias",
    "description": "Bias found in the forensic fingerprint examination process.",
    "author": [
      {
        "name": "Sherry Huang",
        "url": {}
      }
    ],
    "date": "2021-06-30",
    "categories": [],
    "contents": "\nBackground\nFingerprints can be critical evidence in finding someone guilty or innocent. However, the process, though much more advanced now, can still be fraught with bias and mistakes. There are many places where these mistakes can come from. For example, the final decision of the examination process is determined by humans, but the bias could come from people other than the examiner.\nThe forensic proficiency exams used in the studies are meant to mimic the real forensic fingerprint examination process. First, the examiner is shown the latent print. This is the print that is taken from the crime scene and is usually of a lower quality. The examiner then marks minutiae and any noticeable marks that would help them make any determinations later. Exemplar prints are then shown to the examiner as well. Exemplar prints are ones that are taken under ideal conditions and much higher quality. Usually a machine/database (AFIS) is used to gather a group of exemplar prints that are similar to the latent (according to AFIS), and the examiner determines if any of the exemplars matches the latent.\nHowever, the examiner is shown the latent and the exemplars at the same time. This could lead to a form of bias where examiners could mark minutiae on the exemplars in order to match the latent and conclude an individualization, or exclude them as a match. This kind of error can be done intentionally or by mistake. Let’s say an examiner found \\(8\\) noticeable minutiae in the exemplar print, and then let’s say the latent print has some areas of low clarity. Let’s then say the examiner found \\(4\\) matching minutiae in the latent print, but the remaining \\(4\\) minutiae would be found in the poor-quality areas. The examiner could then say there was a match between the prints, despite the fact that technically, only \\(50\\%\\) of the minutiae definitely match.\nBelow is a scatter plot of the latent and exemplar minutiae count from the White Box study (see Data Sets >> White Box for more information). The number of latent minutiae is plotted on the x-axis and the number of exemplar minutiae is plotted on the y-axis. The different colors represent different difficulties the examiner rated the prints, and the lines plot the relationship between latent and exemplar minutiae count, split by difficulty.\n\n\nShow code\n\nggplot(data = wb_determinations, mapping = aes(x = latent_minutiae, y = exemplar_minutiae,\n                                               color = difficulty)) +\n  geom_jitter(alpha = 0.5) +\n  geom_smooth(se = FALSE) +\n  scale_color_brewer(palette = \"Spectral\", na.value = \"gray\") +\n  labs(title = \"Latent vs Exemplar Minutiae Count\", subtitle = \"Sherry Huang, June 2021\",\n       x = \"latent minutiae (#)\", y = \"exemplar minutiae (#)\") +\n  my_theme\n\n\n\n\nThough many of the points are all clustered at the bottom left of the graph, the lines indicate that many of the difficulties have a positive, almost-linear relationship. This means that as the number of latent minutiae increases, so does the number of exemplar minutiae. What’s even more surprising is that the scale for the latent minutiae is larger than the scale for exemplar minutiae. On average, we would expect less minutiae found in latent prints, as they are collected under not ideal conditions, making them less clear and harder to identify. Because of these reasons, I wanted to further explore this potential source of bias.\nTo explore this potential linear relationship a little more, I decided to plot the same variables, but filtering it by the Pair_ID (again, see Data Sets >> White Box for more information). These latent and exemplar pairs are examined by multiple different examiners.\n\n\nShow code\n\nm010003 <- wb_determinations %>% filter(pair_id == \"M010003\") %>%\n  mutate(latent_value_change = case_when(\n    latent_value_analysis == latent_value_comparison ~ 0,\n    latent_value_analysis != latent_value_comparison ~ 1))\n\nggplot(data = m010003, aes(x = latent_minutiae, y = exemplar_minutiae)) +\n  geom_point(mapping = aes(color = difficulty)) +\n  geom_smooth(se = FALSE, color = \"gray\", method = \"lm\") +\n  labs(title = \"Latent vs Exemplar Minutiae for M010003\", subtitle = \"Sherry Huang, June 2021\",\n       x = \"latent minutiae (#)\", y = \"exemplar minutiae (#)\") +\n  my_theme\n\n\n\n\nHere, instead of a smooth line, I fit a linear model instead. When we look at the numerical summary of this model-fitting, we find some surprising results.\n\n\nShow code\n\nm010003_lm <- lm(data = m010003, exemplar_minutiae ~ latent_minutiae)\nsummary(m010003_lm)\n\n\n\nCall:\nlm(formula = exemplar_minutiae ~ latent_minutiae, data = m010003)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-9.231 -4.078  1.769  3.127  7.969 \n\nCoefficients:\n                Estimate Std. Error t value Pr(>|t|)   \n(Intercept)       5.9837     4.9950   1.198  0.25232   \nlatent_minutiae   0.7177     0.1755   4.089  0.00128 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 5.39 on 13 degrees of freedom\nMultiple R-squared:  0.5626,    Adjusted R-squared:  0.529 \nF-statistic: 16.72 on 1 and 13 DF,  p-value: 0.001279\n\nWe can see that the linear model accounts for approximately \\(52.9\\%\\) of the variability in our data, and that the model is in fact statistically significant. Our linear model fits the data pretty well, suggesting that the relationship between latent and exemplar minutiae is positive and linear. So, suspiciously, this data shows that as latent minutiae increases, so does exemplar minutiae. There are \\(320\\) distinct pair ID’s so I didn’t plot all of them, but out of the \\(12\\) I did plot, I discovered that \\(9\\) of them had statistically significant linear models.\n\n\n\n",
    "preview": "posts/2021-06-30-bias/bias_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2021-06-30T16:35:49-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-06-30-tidy-tuesday/",
    "title": "Tidy Tuesday",
    "description": "A display of my Tidy Tuesday visualizations (so far)",
    "author": [
      {
        "name": "Sherry Huang",
        "url": {}
      }
    ],
    "date": "2021-06-30",
    "categories": [],
    "contents": "\nIntroduction: What is Tidy Tuesday?\nTidy Tuesday is “[a] weekly data project aimed at the R ecosystem.” Each week, different (very clean) data sets are released, and anyone who is interested can download the data set and use R in order to make visualizations based on the data. You can find the GitHub here.\n6/15/2021: WEB DuBois & Juneteenth\nW.E.B. DuBois was known for many things, and among them includes his data visualizations. Impressively done by hand and helped to show the disparity between white and Black treatment even after the liberation of slaves, they are the focus of this week’s Tidy Tuesday. One of W.E.B. DuBois’s famous graphs (below) include this line plot depicting Georgia’s white and Black population.\nCumulative White and Black Population in GeorgiaMy recreation, though quite a bit different from the original, can be seen below:\n\n\n\n6/29/2021: Animal Rescues\n“The London Fire Brigade responds to hundreds of requests to rescue animals each year. Its monthly-updated spreadsheet of such events goes back to 2009; it lists the location and type of property, the kind of animal and rescue, hours spent, a (very) brief description, and more. The data is provided from January 2009 and is updated monthly. A range of information is supplied for each incident including some location information (postcode, borough, ward), as well as the data/time of the incidents. We do not routinely record data about animal deaths or injuries.” Source\n\n incident_number     date_time_of_call     cal_year   \n Min.   :     4149   Length:6550        Min.   :2009  \n 1st Qu.: 51778111   Class :character   1st Qu.:2012  \n Median : 92835641   Mode  :character   Median :2015  \n Mean   : 94594076                      Mean   :2015  \n 3rd Qu.:135229884                      3rd Qu.:2018  \n Max.   :233284091                      Max.   :2021  \n NA's   :2908                                         \n   fin_year         type_of_incident    pump_count       \n Length:6550        Length:6550        Length:6550       \n Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character  \n                                                         \n                                                         \n                                                         \n                                                         \n pump_hours_total   hourly_notional_cost incident_notional_cost\n Length:6550        Min.   :260.0        Length:6550           \n Class :character   1st Qu.:260.0        Class :character      \n Mode  :character   Median :298.0        Mode  :character      \n                    Mean   :300.8                              \n                    3rd Qu.:333.0                              \n                    Max.   :352.0                              \n                                                               \n final_description  animal_group_parent originof_call     \n Length:6550        Length:6550         Length:6550       \n Class :character   Class :character    Class :character  \n Mode  :character   Mode  :character    Mode  :character  \n                                                          \n                                                          \n                                                          \n                                                          \n property_type      property_category  special_service_type_category\n Length:6550        Length:6550        Length:6550                  \n Class :character   Class :character   Class :character             \n Mode  :character   Mode  :character   Mode  :character             \n                                                                    \n                                                                    \n                                                                    \n                                                                    \n special_service_type  ward_code             ward          \n Length:6550          Length:6550        Length:6550       \n Class :character     Class :character   Class :character  \n Mode  :character     Mode  :character   Mode  :character  \n                                                           \n                                                           \n                                                           \n                                                           \n borough_code         borough          stn_ground_name   \n Length:6550        Length:6550        Length:6550       \n Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character  \n                                                         \n                                                         \n                                                         \n                                                         \n     uprn              street              usrn          \n Length:6550        Length:6550        Length:6550       \n Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character  \n                                                         \n                                                         \n                                                         \n                                                         \n postcode_district   easting_m          northing_m       \n Length:6550        Length:6550        Length:6550       \n Class :character   Class :character   Class :character  \n Mode  :character   Mode  :character   Mode  :character  \n                                                         \n                                                         \n                                                         \n                                                         \n easting_rounded  northing_rounded    latitude       longitude     \n Min.   :504050   Min.   :157050   Min.   :51.30   Min.   :-0.499  \n 1st Qu.:524750   1st Qu.:175050   1st Qu.:51.46   1st Qu.:-0.216  \n Median :531650   Median :181350   Median :51.52   Median :-0.104  \n Mean   :531250   Mean   :180720   Mean   :51.51   Mean   :-0.111  \n 3rd Qu.:537750   3rd Qu.:186750   3rd Qu.:51.57   3rd Qu.:-0.013  \n Max.   :571350   Max.   :200750   Max.   :51.69   Max.   : 0.466  \n                                   NA's   :3380    NA's   :3380    \n\n",
    "preview": "posts/2021-06-30-tidy-tuesday/tidy-tuesday_files/figure-html5/unnamed-chunk-1-1.png",
    "last_modified": "2021-07-01T10:58:20-04:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-06-28-data-wrangling/",
    "title": "Data Wrangling",
    "description": "A short tutorial on tidy data in R!",
    "author": [
      {
        "name": "Sherry Huang",
        "url": {}
      }
    ],
    "date": "2021-06-28",
    "categories": [],
    "contents": "\nTidy Data\nTidy data is the easiest data format to use with R. A data set is tidy if:\nEach variable is in its own column\nEach observation is in its own row\nEach value is in its own cell\nThe below data set is an example of tidy data.\n\n\nShow code\n\ntable1\n\n\n# A tibble: 6 x 4\n  country      year  cases population\n  <chr>       <int>  <int>      <int>\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\nTidy data is easier to use because…\n1. Extracting variables\nExtracting variables (and information about them) becomes easier when your data is tidy. For example, extracting the mean of the cases from Table 1 or the mean of the latent minutiae from the White Box Determinations data set is easy when you have tidy data.\n\n\nShow code\n\nmean(table1$cases)\n\n\n[1] 91276.67\n\nShow code\n\nmean(wb_determinations$`Latent minutiae`)\n\n\n[1] 12.04853\n\nAn untidy form of Table 1 (let’s call this Table 2) would make this more difficult; see below. It contains the same information as Table 1, but instead of a cases column and a population column, those are all put into its own column and their corresponding count/value of cases/population is also put in its own column.\n\n\nShow code\n\ntable2\n\n\n# A tibble: 12 x 4\n   country      year type            count\n   <chr>       <int> <chr>           <int>\n 1 Afghanistan  1999 cases             745\n 2 Afghanistan  1999 population   19987071\n 3 Afghanistan  2000 cases            2666\n 4 Afghanistan  2000 population   20595360\n 5 Brazil       1999 cases           37737\n 6 Brazil       1999 population  172006362\n 7 Brazil       2000 cases           80488\n 8 Brazil       2000 population  174504898\n 9 China        1999 cases          212258\n10 China        1999 population 1272915272\n11 China        2000 cases          213766\n12 China        2000 population 1280428583\n\nIn this form, extracting the mean cases is more difficult.\n\n\nShow code\n\nmean(table2$count[c(1, 3, 5, 7, 9, 11)])\n\n\n[1] 91276.67\n\nNot only is this code more difficult to write, but it’s also less reusable.\n2. Reusable code\nIf we wanted to find the mean population instead of cases in Table 1, we can easily reuse the above code chunk mean(table1$cases) or mean(wb_determinations$Exemplar minutiae).\n\n\nShow code\n\nmean(table1$population)\n\n\n[1] 490072924\n\nShow code\n\nmean(wb_determinations$`Exemplar minutiae`)\n\n\n[1] 8.205094\n\nHowever, that’s (again) not as easy for Table 2, where the original code was mean(table2$count[c(1, 3, 5, 7, 9, 11)]).\n\n\nShow code\n\nmean(table2$count[c(2, 4, 6, 8, 10, 12)])\n\n\n[1] 490072924\n\n3. Calculations\nBecause of the ease of extracting and reusing tidy data, this can make calculations easier. For example, consider calculating ratios of the variables cases and population each year for Table 1 vs Table 2.\n\n\nShow code\n\n(table1$cases) / (table1$population)\n\n\n[1] 0.0000372741 0.0001294466 0.0002193930 0.0004612363 0.0001667495\n[6] 0.0001669488\n\nShow code\n\n(table2$count[c(1, 3, 5, 7, 9, 11)]) / (table2$count[c(2, 4, 6, 8, 10, 12)])\n\n\n[1] 0.0000372741 0.0001294466 0.0002193930 0.0004612363 0.0001667495\n[6] 0.0001669488\n\nTo summarize:\nR stores each data frame as a list of column vectors, making it easier to extract a variable if your data is tidy.\nR computes many functions and operations in a vectorized fashion. Most functions in R will expect your data to be organized into a tidy format.\nUntidy Data\nWide Data\nHow you tidy an untidy data set will depend on the initial configuration of the data! Below is Table 4a, which shows a different configuration of part of the information from Table 1.\n\n\nShow code\n\ntable4a\n\n\n# A tibble: 3 x 3\n  country     `1999` `2000`\n* <chr>        <int>  <int>\n1 Afghanistan    745   2666\n2 Brazil       37737  80488\n3 China       212258 213766\n\nThe variables here are country, year, and cases, but that’s not how it’s represented in the above table. gather() in the tidyr package (which comes with tidyverse) converts wide data (above) to long data. gather() returns a tidy copy of the data set but doesn’t leave the original, so you have to save the copy in order to use it later.\nkey: name of a new column that contains former column names\nvalue: name of a new column that contains former cell values\nnumbers: tells gather() which columns to use to build the new columns\n\n\nShow code\n\ntidytable4a <- table4a %>% gather(key = \"year\", value = \"cases\", 2, 3, convert = TRUE)\ntidytable4a\n\n\n# A tibble: 6 x 3\n  country      year  cases\n  <chr>       <int>  <int>\n1 Afghanistan  1999    745\n2 Brazil       1999  37737\n3 China        1999 212258\n4 Afghanistan  2000   2666\n5 Brazil       2000  80488\n6 China        2000 213766\n\nconvert = TRUE in the code allows each new column to be converted to an appropriate data type.\nNarrow Data\nNarrow data uses a literal key column and a literal value column to store multiple values. Table 2 below is a good example.\n\n\nShow code\n\ntable2\n\n\n# A tibble: 12 x 4\n   country      year type            count\n   <chr>       <int> <chr>           <int>\n 1 Afghanistan  1999 cases             745\n 2 Afghanistan  1999 population   19987071\n 3 Afghanistan  2000 cases            2666\n 4 Afghanistan  2000 population   20595360\n 5 Brazil       1999 cases           37737\n 6 Brazil       1999 population  172006362\n 7 Brazil       2000 cases           80488\n 8 Brazil       2000 population  174504898\n 9 China        1999 cases          212258\n10 China        1999 population 1272915272\n11 China        2000 cases          213766\n12 China        2000 population 1280428583\n\nThe column type contains key names, and the column count is the value column. To fix this, we can use the spread() function (also in the tidyr package). It gives each unique value in the key column its own column, and the name of the value becomes the column name. It will then redistribute the values in the value column across the new columns.\n\n\nShow code\n\ntidytable2 <- table2 %>% spread(key = type, value = count)\ntidytable2\n\n\n# A tibble: 6 x 4\n  country      year  cases population\n  <chr>       <int>  <int>      <int>\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\nWe use quotations in the gather() function code because the variables year and cases did not exist yet in Table 4a. But they do already exist in Table 2 (in the form of value names), so we don’t need to use quotation marks when using the spread() function.\nIt’s not always clear whether or not a data set is tidy; tidiness depends on the variables in your data set/what you want to explore.\nSeparating Columns\nSometimes, one column contains more information that we need. Take Table 3 below, for example.\n\n\nShow code\n\ntable3\n\n\n# A tibble: 6 x 3\n  country      year rate             \n* <chr>       <int> <chr>            \n1 Afghanistan  1999 745/19987071     \n2 Afghanistan  2000 2666/20595360    \n3 Brazil       1999 37737/172006362  \n4 Brazil       2000 80488/174504898  \n5 China        1999 212258/1272915272\n6 China        2000 213766/1280428583\n\nrate consists of cases/population, and it would be nice if we could separate those values!\n\n\nShow code\n\nsep_table3 <- table3 %>% separate(col = rate, into = c(\"cases\", \"population\"), sep = \"/\", convert = TRUE, remove = FALSE)\nsep_table3\n\n\n# A tibble: 6 x 5\n  country      year rate               cases population\n  <chr>       <int> <chr>              <int>      <int>\n1 Afghanistan  1999 745/19987071         745   19987071\n2 Afghanistan  2000 2666/20595360       2666   20595360\n3 Brazil       1999 37737/172006362    37737  172006362\n4 Brazil       2000 80488/174504898    80488  174504898\n5 China        1999 212258/1272915272 212258 1272915272\n6 China        2000 213766/1280428583 213766 1280428583\n\nUniting Columns\nSometimes, you want to unite data. For example, if we only had cases and population, we can make a rate column by uniting the two variables.\n\n\nShow code\n\ntable1\n\n\n# A tibble: 6 x 4\n  country      year  cases population\n  <chr>       <int>  <int>      <int>\n1 Afghanistan  1999    745   19987071\n2 Afghanistan  2000   2666   20595360\n3 Brazil       1999  37737  172006362\n4 Brazil       2000  80488  174504898\n5 China        1999 212258 1272915272\n6 China        2000 213766 1280428583\n\n\n\nShow code\n\nunite_table1 <- table1 %>%\n  unite(col = \"rate\", cases, population, sep = \":\", remove = FALSE)\nunite_table1\n\n\n# A tibble: 6 x 5\n  country      year rate               cases population\n  <chr>       <int> <chr>              <int>      <int>\n1 Afghanistan  1999 745:19987071         745   19987071\n2 Afghanistan  2000 2666:20595360       2666   20595360\n3 Brazil       1999 37737:172006362    37737  172006362\n4 Brazil       2000 80488:174504898    80488  174504898\n5 China        1999 212258:1272915272 212258 1272915272\n6 China        2000 213766:1280428583 213766 1280428583\n\nJoining Data Sets\nThis is useful for when you have multiple data sets that are related to each other. Sometimes the data will be easier to analyze if you join them all into a single table. The below functions come in the dplyr package (which is included in the tidyverse package). We will be using \\(3\\) data sets, shown below:\n\n\nShow code\n\nband_members\n\n\n# A tibble: 3 x 2\n  name  band   \n  <chr> <chr>  \n1 Mick  Stones \n2 John  Beatles\n3 Paul  Beatles\n\nShow code\n\nband_instruments\n\n\n# A tibble: 3 x 2\n  name  plays \n  <chr> <chr> \n1 John  guitar\n2 Paul  bass  \n3 Keith guitar\n\nShow code\n\nband_instruments2\n\n\n# A tibble: 3 x 2\n  artist plays \n  <chr>  <chr> \n1 John   guitar\n2 Paul   bass  \n3 Keith  guitar\n\nleft_join(), right_join(), full_join(), inner_join(): augment a copy of one data frame with information from a second\nleft_join(): retains all the rows of the first data set and only adds rows from the second data set that match rows in the first. In the example below, band_members is the first data set, and band_instruments is the second data set, so the below code will keep everything from band_members and include information from band_instruments.\n\n\n\nShow code\n\nleft_band <- band_members %>% left_join(band_instruments, by = \"name\")\nleft_band\n\n\n# A tibble: 3 x 3\n  name  band    plays \n  <chr> <chr>   <chr> \n1 Mick  Stones  <NA>  \n2 John  Beatles guitar\n3 Paul  Beatles bass  \n\nright_join(): retains every row from second data set and only adds rows from the first data set that have a match in the second. In the example below, band_members is still the first data set and band_instruments is the second data set, so the code below keeps band_instruments and includes information from band_members.\n\n\nShow code\n\nright_band <- band_members %>% right_join(band_instruments, by = \"name\")\nright_band\n\n\n# A tibble: 3 x 3\n  name  band    plays \n  <chr> <chr>   <chr> \n1 John  Beatles guitar\n2 Paul  Beatles bass  \n3 Keith <NA>    guitar\n\nfull_join(): retains every row from each data sets\n\n\nShow code\n\nfull_band <- band_members %>% full_join(band_instruments, by = \"name\")\nfull_band\n\n\n# A tibble: 4 x 3\n  name  band    plays \n  <chr> <chr>   <chr> \n1 Mick  Stones  <NA>  \n2 John  Beatles guitar\n3 Paul  Beatles bass  \n4 Keith <NA>    guitar\n\ninner_join(): only retains rows that appear in both data sets\n\n\nShow code\n\ninner_band <- band_members %>% inner_join(band_instruments, by = \"name\")\ninner_band\n\n\n# A tibble: 2 x 3\n  name  band    plays \n  <chr> <chr>   <chr> \n1 John  Beatles guitar\n2 Paul  Beatles bass  \n\nNote: If the variables are named differently in the two data sets, you can still use the join functions by changing the code a little bit. Below is an example using the full_join() function; in band_instruments2, the name variable is called artist instead.\n\n\nShow code\n\nfull_band_var <- band_members %>% full_join(band_instruments2, by = c(\"name\" = \"artist\"))\nfull_band_var\n\n\n# A tibble: 4 x 3\n  name  band    plays \n  <chr> <chr>   <chr> \n1 Mick  Stones  <NA>  \n2 John  Beatles guitar\n3 Paul  Beatles bass  \n4 Keith <NA>    guitar\n\nsemi_join(), anti_join(): filters the contents of one data frame against the contents of a second\nsemi_join(): returns every row in the first data set that has a match in the second data set, useful shortcut for complicated filtering. Does not add information from the second data set.\n\n\n\nShow code\n\nsemi_band <- band_members %>% semi_join(band_instruments, by = \"name\")\nsemi_band\n\n\n# A tibble: 2 x 2\n  name  band   \n  <chr> <chr>  \n1 John  Beatles\n2 Paul  Beatles\n\nanti_join(): returns all the rows in the first data set that do not have a match in the second data set, useful way to check for possible errors in a join. Also does not include information from the second data set.\n\n\nShow code\n\nanti_band <- band_members %>% anti_join(band_instruments, by = \"name\")\nanti_band\n\n\n# A tibble: 1 x 2\n  name  band  \n  <chr> <chr> \n1 Mick  Stones\n\nbind_rows(), bind_cols(), set operations: combine data sets in more simple ways\nbind_cols(): combines data sets that contain the same observations in the same order\nbind_rows(): combines data sets that contain same variables but different observations\nunion(): returns every row that appears in either data set but removes duplicate copies of the rows\nintersect(): returns only rows that appear in both data sets (removes duplicate copies of these rows)\nsetdiff(): returns all rows that appear in the first data set but not the second\n\ndistinct(): returns distinct values of a column\n\n\nShow code\n\nband_instruments %>% distinct(plays)\n\n\n# A tibble: 2 x 1\n  plays \n  <chr> \n1 guitar\n2 bass  \n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-06-28T15:25:18-04:00",
    "input_file": {}
  }
]
